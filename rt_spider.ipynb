{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile rt_index_spider.py\n",
    "import configparser\n",
    "import sys,os\n",
    "from rt_parser import rt_parser\n",
    "from rt_output import rt_output\n",
    "from log_manager import log_manager\n",
    "sys.path.append('/home/guijideanhao/pyproject/scrapy_toolv2')\n",
    "from html_downloader import html_downloader\n",
    "\n",
    "CONF = configparser.ConfigParser()\n",
    "CONF.read('rt_config.cfg')\n",
    "DATAPATH = CONF.get('DEFAULT','datapath')\n",
    "TABLE_NAME = CONF.get('INDEX','table_name')\n",
    "DBPATH =CONF.get('INDEX','dbpath')\n",
    "LOGGING_FILE = CONF.get('INDEX','logging_file')\n",
    "\n",
    "class rt_index_spider():\n",
    "    '''\n",
    "    organizer for scrape index page\n",
    "    Attributes:\n",
    "        \n",
    "    '''\n",
    "    def __init__(self,datapath=DATAPATH, logging_file=LOGGING_FILE, dbpath=DBPATH):\n",
    "        self.hd = html_downloader(china=False)\n",
    "        self.lg = log_manager(os.path.join(datapath,logging_file))\n",
    "        self.parser = rt_parser()\n",
    "        self.output = rt_output(os.path.join(datapath,dbpath))\n",
    "        \n",
    "        self.usedurl = self.lg.get_info_list(success_tag='SUCCESS')\n",
    "    def scrape(self,table_name=TABLE_NAME, teststop=-1):\n",
    "        i = 0\n",
    "        while True:\n",
    "            i = i+1\n",
    "            url = 'https://www.rottentomatoes.com/api/private/v2.0/browse?sortBy=release&type=dvd-streaming-all&page={}'.format(i+1)\n",
    "            if url in self.usedurl:\n",
    "                continue\n",
    "            try:\n",
    "                res = self.hd.request_proxy(url)\n",
    "                if res == None:\n",
    "                    self.lg.write_log(info=url,success=False,info_type='download1')\n",
    "                    break\n",
    "            except:\n",
    "                self.lg.write_log(info=url,success=False,info_type='download2')\n",
    "                break\n",
    "            try:\n",
    "                success,*df = self.parser.parse_index(res.content)\n",
    "                if not success:\n",
    "                    self.lg.write_log(info=url,success=False,info_type='parse1')\n",
    "                    break\n",
    "            except:\n",
    "                self.lg.write_log(info=url,success=False,info_type='parse2')\n",
    "                break\n",
    "            try:\n",
    "                self.output.output_index(df[0], table_name)\n",
    "                self.lg.write_log(info=url,success=True,info_type='success')\n",
    "            except:\n",
    "                self.lg.write_log(info=url,success=False,info_type='db')\n",
    "                break\n",
    "            if teststop==0:\n",
    "                print('test end')\n",
    "                break\n",
    "            if teststop>0:\n",
    "                teststop=teststop-1\n",
    "        print('mission complete')\n",
    "                \n",
    "if __name__ == '__main__':\n",
    "#     # test code\n",
    "#     test = rt_index_spider(datapath=CONF.get('TEST','datapath'))\n",
    "#     test.scrape(teststop=2)\n",
    "    # formal code\n",
    "    spider = rt_index_spider()\n",
    "    spider.scrape()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile rt_page_spider.py\n",
    "import configparser\n",
    "import sys,os,sqlite3\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin\n",
    "from rt_parser import rt_parser\n",
    "from rt_output import rt_output\n",
    "from log_manager import log_manager\n",
    "sys.path.append('/home/guijideanhao/pyproject/scrapy_toolv2')\n",
    "from html_downloader import html_downloader\n",
    "from rt_index_spider import rt_index_spider\n",
    "CONF = configparser.ConfigParser()\n",
    "CONF.read('rt_config.cfg')\n",
    "DATAPATH = CONF.get('DEFAULT','datapath')\n",
    "TABLE_NAME = CONF.get('PAGE','table_name')\n",
    "TABLE_NAME2 = CONF.get('PAGE','table_name2')\n",
    "TABLE_NAME3 = CONF.get('PAGE','table_name3')\n",
    "DBPATH =CONF.get('PAGE','dbpath')\n",
    "LOGGING_FILE = CONF.get('PAGE','logging_file')\n",
    "DOMAIN_URL = CONF.get('DEFAULT', 'domain_url')\n",
    "index_table_name = CONF.get('INDEX', 'table_name')\n",
    "index_dbpath_full = os.path.join(DATAPATH,DBPATH)\n",
    "index_conn = sqlite3.connect(index_dbpath_full)\n",
    "\n",
    "URLLIST = pd.read_sql('select url from {}'.format(index_table_name), index_conn)['url'].values.tolist()\n",
    "index_conn.close()\n",
    "\n",
    "class rt_page_spider(rt_index_spider):\n",
    "    def __init__(self,datapath=DATAPATH, logging_file=LOGGING_FILE, dbpath=DBPATH):\n",
    "        rt_index_spider.__init__(self, datapath, logging_file, dbpath)\n",
    "    def scrape(self,urllist=URLLIST, table_name=TABLE_NAME, teststop=-1):\n",
    "        for url in urllist:\n",
    "            if url in self.usedurl:\n",
    "                continue\n",
    "            if teststop==0:\n",
    "                print('test end')\n",
    "                break\n",
    "            if teststop>0:\n",
    "                teststop=teststop-1\n",
    "            full_url = urljoin(DOMAIN_URL,url)\n",
    "            try:\n",
    "                res = self.hd.request_proxy(full_url)\n",
    "                if res == None:\n",
    "                    self.lg.write_log(info=url,success=False,info_type='download1')\n",
    "                    continue\n",
    "            except:\n",
    "                self.lg.write_log(info=url,success=False,info_type='download2')\n",
    "                continue\n",
    "            try:\n",
    "                success,*df = self.parser.parse_page(url,res.content)\n",
    "                if not success:\n",
    "                    self.lg.write_log(info=url,success=False,info_type='parse1')\n",
    "                    continue\n",
    "            except:\n",
    "                self.lg.write_log(info=url,success=False,info_type='parse2')\n",
    "                continue\n",
    "            try:\n",
    "                self.output.output_page(df[0], table_name)\n",
    "                self.lg.write_log(info=url,success=True,info_type='success')\n",
    "            except:\n",
    "                self.lg.write_log(info=url,success=False,info_type='db')\n",
    "                continue\n",
    "        print('mission complete')\n",
    "if __name__ == '__main__':\n",
    "#     #test code\n",
    "    \n",
    "#     urllist = ['/m/to_be_and_to_have', '/m/the_living_end', '/m/the_blood_of_a_poet']\n",
    "#     test = rt_page_spider(datapath=CONF.get('TEST','datapath'))\n",
    "#     test.scrape(urllist, teststop=2)\n",
    "    \n",
    "    # formal code\n",
    "    spider = rt_page_spider()\n",
    "    spider.scrape(table_name=TABLE_NAME3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting rt_critic_review_spider.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile rt_critic_review_spider.py\n",
    "import configparser\n",
    "import sys,os,sqlite3\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin\n",
    "from rt_parser import rt_parser\n",
    "from rt_output import rt_output\n",
    "from log_manager import log_manager\n",
    "sys.path.append('/home/guijideanhao/pyproject/scrapy_toolv2')\n",
    "from html_downloader import html_downloader\n",
    "from rt_index_spider import rt_index_spider\n",
    "CONF = configparser.ConfigParser()\n",
    "CONF.read('rt_config.cfg')\n",
    "DATAPATH = CONF.get('DEFAULT','datapath')\n",
    "TABLE_NAME = CONF.get('CRITIC_REVIEW','table_name')\n",
    "DBPATH =CONF.get('CRITIC_REVIEW','dbpath')\n",
    "LOGGING_FILE = CONF.get('CRITIC_REVIEW','logging_file')\n",
    "DOMAIN_URL = CONF.get('DEFAULT', 'domain_url')\n",
    "index_table_name = CONF.get('INDEX', 'table_name')\n",
    "index_dbpath_full = os.path.join(DATAPATH,DBPATH)\n",
    "index_conn = sqlite3.connect(index_dbpath_full)\n",
    "\n",
    "URLLIST = pd.read_sql('select url from {}'.format(index_table_name), index_conn)['url'].values.tolist()\n",
    "index_conn.close()\n",
    "\n",
    "class rt_critic_review_spider(rt_index_spider):\n",
    "    def __init__(self,datapath=DATAPATH, logging_file=LOGGING_FILE, dbpath=DBPATH):\n",
    "        rt_index_spider.__init__(self, datapath, logging_file, dbpath)\n",
    "    def scrape(self,urllist=URLLIST, table_name=TABLE_NAME, teststop=-1):\n",
    "        for url in urllist:\n",
    "            if url in self.usedurl:\n",
    "                continue\n",
    "            if teststop==0:\n",
    "                print('test end')\n",
    "                break\n",
    "            if teststop>0:\n",
    "                teststop=teststop-1\n",
    "            self.scrape_single_url(url)\n",
    "        print('mission complete')\n",
    "    def scrape_single_url(self, url,pageurl=None,table_name=TABLE_NAME):\n",
    "        if pageurl:\n",
    "            full_url = urljoin(DOMAIN_URL, pageurl)\n",
    "        else:\n",
    "            full_url = urljoin(DOMAIN_URL, url)+'/reviews'\n",
    "#         print(full_url)\n",
    "        try:\n",
    "            res = self.hd.request_proxy(full_url)\n",
    "            \n",
    "            if res == None:\n",
    "                self.lg.write_log(info=url,success=False,info_type='download1')\n",
    "                return\n",
    "#             print(res.status_code)\n",
    "        except:\n",
    "            self.lg.write_log(info=url,success=False,info_type='download2')\n",
    "            return\n",
    "        try:\n",
    "            success,*df = self.parser.parse_critic_review(url,res.content)\n",
    "#             print(success)\n",
    "            if not success:\n",
    "                self.lg.write_log(info=url,success=False,info_type='parse1')\n",
    "                return\n",
    "        except:\n",
    "            self.lg.write_log(info=url,success=False,info_type='parse2')\n",
    "            return\n",
    "        try:\n",
    "            self.output.output_critic_review(df[0], table_name)\n",
    "#             print('saved')\n",
    "        except:\n",
    "            self.lg.write_log(info=url,success=False,info_type='db')\n",
    "            return\n",
    "\n",
    "        next_page, *pageurls = self.parser.parse_critic_review_next(url,res.content)\n",
    "#         print(next_page)\n",
    "#         print(pageurls)\n",
    "        if next_page:\n",
    "            self.scrape_single_url(url=url, pageurl=pageurls[0], table_name=TABLE_NAME)\n",
    "        else:\n",
    "            self.lg.write_log(info=url,success=True,info_type='success')\n",
    "            return\n",
    "if __name__ == \"__main__\":\n",
    "    # test\n",
    "#     urllist = ['/m/to_the_stars']\n",
    "#     test = rt_critic_review_spider(datapath=CONF.get('TEST','datapath'))\n",
    "#     test.scrape_single_url(urllist[0])\n",
    "    # formal\n",
    "    spider = rt_critic_review_spider()\n",
    "    spider.scrape(table_name=TABLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
